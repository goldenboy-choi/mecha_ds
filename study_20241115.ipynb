{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: (300404, 3)\n",
      "train size: (198879, 3)\n",
      "test size: (101525, 3)\n",
      "solution size: (101525, 3)\n",
      "weather size: (113880, 8)\n",
      "weather train size: (75816, 8)\n",
      "weather test size: (38064, 8)\n",
      "113880\n",
      "113880\n",
      "weather valid size: (672, 3)\n",
      "672\n",
      "temperature size: (8760, 52)\n",
      "8760\n",
      "building_metadata size: (1449, 6)\n"
     ]
    }
   ],
   "source": [
    "# train 데이터는 '2019-01-01' ~ '2019-08-31'\n",
    "# test 데이터는  '2019-09-01' ~ '2019-12-31'\n",
    "\n",
    "data = pd.read_csv('data.csv', parse_dates=['datetime'])\n",
    "train = data.set_index('datetime').loc['2019-01-01':'2019-08-31'].reset_index()\n",
    "test = data.set_index('datetime').loc['2019-09-01':'2019-12-31'].reset_index()\n",
    "solution = pd.read_csv('data_solution.csv', parse_dates=['datetime'])\n",
    "\n",
    "building = pd.read_csv('building_metadata.csv')\n",
    "weather = pd.read_csv('weather.csv', parse_dates=['datetime'])\n",
    "weather_train = weather.set_index('datetime').loc['2019-01-01':'2019-08-31'].reset_index()\n",
    "weather_test = weather.set_index('datetime').loc['2019-09-01':'2019-12-31'].reset_index()\n",
    "weather_valid = pd.read_csv('weather_valid.csv', parse_dates=['datetime'])\n",
    "\n",
    "temperature = pd.read_csv('temperature.csv', parse_dates=['datetime'])\n",
    "\n",
    "# 데이터 체크\n",
    "print('data size:', data.shape)\n",
    "print('train size:', train.shape)\n",
    "print('test size:', test.shape)\n",
    "print('solution size:', solution.shape)\n",
    "\n",
    "print('weather size:', weather.shape)\n",
    "print('weather train size:', weather_train.shape)\n",
    "print('weather test size:', weather_test.shape)\n",
    "print(weather_train.shape[0] + weather_test.shape[0])\n",
    "print(365 * 24 * 13)\n",
    "\n",
    "print('weather valid size:', weather_valid.shape)\n",
    "print(28 * 24)\n",
    "\n",
    "print('temperature size:', temperature.shape)\n",
    "print(365 * 24)\n",
    "\n",
    "print('building_metadata size:', building.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 0.416\n"
     ]
    }
   ],
   "source": [
    "# Problem 1\n",
    "\n",
    "# Nan인 데이터 값들을 채워넣는 과정이다.\n",
    "\n",
    "# Restore를 수행할 column들\n",
    "columns_to_restore = ['temperature','dew_point','cloud','rain_hourly','air_pressure','wind_direction']\n",
    "\n",
    "# train과 test 날씨 데이터를 결합한다.\n",
    "weather_df = pd.concat([weather_train, weather_test])\n",
    "\n",
    "# region_id와 datetime으로 인덱스를 설정하여 탐색 속도를 높인다.\n",
    "weather_df = weather_df.set_index(['region_id', 'datetime']).sort_index()\n",
    "\n",
    "# interpolation을 이용하여 비어있는 값들을 채워야 한다.\n",
    "# 그렇기 위해서는 먼저 데이터를 time series 데이터로 추출해야 한다.\n",
    "# 각각의 region 마다 독립적인 time series 데이터가 있기 때문에 time series 별로 따로 추출해야 한다.\n",
    "for column in columns_to_restore:\n",
    "    for region_id in range(13):\n",
    "        s = weather_df.loc[region_id, column].copy()\n",
    "        \n",
    "        # interpolation으로 값을 채울때 limit_direction을 both로 선택하여 모든 값들을 채울 수 있도록 한다.\n",
    "        # cloud 칼럼의 경우 0, 2, 4, 6, 8로 이루어진 값이기 때문에 \n",
    "        # 해당 값들로 정수화 하는 과정이 추가로 필요하다.\n",
    "        if column == 'cloud':\n",
    "            interpolation = (s/2).interpolate(limit_direction='both')\n",
    "            interpolation[interpolation.notnull()] = interpolation[interpolation.notnull()].astype(np.int8)*2\n",
    "        else:\n",
    "            interpolation = s.interpolate(limit_direction='both')\n",
    "        weather_df.loc[region_id, column].iloc[:] = interpolation\n",
    "        \n",
    "# 주어진 valid 데이터(weather_valid.csv)를 이용하여 interpolate의 정확도를 테스트한다.\n",
    "# valid 데이터는 region_id = 9 의 2월 데이터가 빠짐없이 들어있다.\n",
    "# 이를 이용하여 interpolate된 값과 correlation을 계산한다.\n",
    "column = columns_to_restore[0]\n",
    "region_id = 9\n",
    "st  = '2019-02-01 00:00:00'\n",
    "end = '2019-02-28 23:59:59'\n",
    "index_slice = slice(st, end)\n",
    "\n",
    "weather_valid_df = weather_valid.copy()\n",
    "weather_valid_df = weather_valid_df.set_index(['region_id', 'datetime'], drop=False).sort_index()\n",
    "\n",
    "# RMSE 값을 계산하여 출력한다.\n",
    "rmse = ((weather_df.loc[region_id, column] - weather_valid_df.loc[region_id, column]) ** 2).mean() ** .5\n",
    "print('Answer:', round(rmse,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8760 54\n",
      "Answer: minneapolis, austin\n"
     ]
    }
   ],
   "source": [
    "# Problem 2\n",
    "\n",
    "# 현재 주어진 데이터의 region은 번호로 주어져 있고 실제 도시 이름을 모른다.\n",
    "# 이를 다른 데이터를 이용하여 실제 도시 이름을 유추해본다.\n",
    "# 이때 다른 데이터와 비교할 metric으로 dynamic time warping (DTW)를 사용한다.\n",
    "# temperature.csv에서는 여러 도시별 시간별 기온이 들어있다.\n",
    "# weather 데이터와 temperature 추가 데이터의 datetime은 같은 표준시에 맞춰져 있기 떄문에 \n",
    "# 따로 datetime를 처리할 필요는 없다.\n",
    "\n",
    "weather_df = weather_df.reset_index()\n",
    "\n",
    "# weather 데이터에서 temperature 데이터를 추출한다.\n",
    "df_temperature_pivot = weather_df.reset_index().pivot_table(index='datetime', columns='region_id', values='temperature')\n",
    "df_temperature_pivot = df_temperature_pivot.loc[:,[0,1]]\n",
    "\n",
    "# 추가 데이터의 temperature 데이터를 가져와서 weather 데이터와 datetime을 이용하여 merge 한다.\n",
    "# 그러기 위해서 양쪽 데이터의 index를 datetime으로 설정해 두어야 한다.\n",
    "# merge 할때 key join 방법으로 inner를 선택한다.\n",
    "temperature_df = temperature.copy()\n",
    "temperature_df.set_index('datetime', inplace=True)\n",
    "temperature_df = temperature_df.merge(df_temperature_pivot, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "print(temperature_df.shape[0], temperature_df.shape[1]+1) # datetime column Count 추가\n",
    "\n",
    "# merge된 데이터에서 weather 데이터와 temperature 추가 데이터를 분리한다.\n",
    "col_regions = [i for i in range(2)]\n",
    "col_cities = [col for col in temperature_df.columns if col not in col_regions]\n",
    "temperature_regions = [temperature_df[col].copy() for col in temperature_df.columns if col in col_regions]\n",
    "temperature_cities = [temperature_df[col].copy() for col in temperature_df.columns if col in col_cities]\n",
    "\n",
    "# RMSE를 계산한다.\n",
    "def calc_rmse(a, b):\n",
    "    return (((a - b)**2).abs()).mean()**(1/2)\n",
    "\n",
    "rmse_table = []\n",
    "for i, temperature_city in enumerate(temperature_cities):\n",
    "    rmse_row = []\n",
    "    for j, temperature_region in enumerate(temperature_regions):\n",
    "        rmse = calc_rmse(temperature_region, temperature_city)\n",
    "        rmse_row.append(rmse)\n",
    "        \n",
    "    rmse_table.append(rmse_row)\n",
    "    \n",
    "df_rmse = np.array(rmse_table)\n",
    "\n",
    "# RMSE가 가장 작은 index및 그 값을 찾아서 dataframe으로 만든다.\n",
    "df_findCity_indicies = df_rmse.argmin(axis=0)\n",
    "df_findCity_rmse = df_rmse.min(axis=0)\n",
    "df_findCity = pd.DataFrame.from_dict({\n",
    "    #'dtw': df_findCity_rmse,\n",
    "    'city': np.array(col_cities)[df_findCity_indicies],\n",
    "    'region': col_regions,\n",
    "})\n",
    "\n",
    "# matching된 city들을 출력한다.\n",
    "print('Answer:', df_findCity['city'][0]+', '+df_findCity['city'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198879, 14)\n",
      "Answer: air_pressure 0.255\n"
     ]
    }
   ],
   "source": [
    "# Problem 3\n",
    "\n",
    "# train 데이터와 building 데이터와 weather 데이터를 merge 한다.\n",
    "# merge 할때 key joint 방법은 'left'를 사용한다.\n",
    "# join 할때 사용할 key 값들로 region_id와 date_time를 사용한다.\n",
    "train_df = train.copy()\n",
    "train_df = train_df.merge(building, on='user_id', how='left')\n",
    "train_df_merged = train_df.merge(weather_df, on=['region_id', 'datetime'], how='left')\n",
    "print(train_df_merged.shape)\n",
    "\n",
    "df = train_df_merged.copy()\n",
    "df['hr'] = df['datetime'].dt.hour\n",
    "df = df.groupby(['region_id', 'hr']).mean()\n",
    "\n",
    "# column은 time에 대해 변하는 변수들만 사용한다. user_id나 area 등 시간에 독립적인 값들은 제외한다.\n",
    "corrs = []\n",
    "df = df.reset_index()\n",
    "columns = ['generate', 'temperature', 'cloud', 'dew_point', 'rain_hourly', 'air_pressure', 'wind_direction']\n",
    "\n",
    "total_corr = np.zeros([len(columns), len(columns)])\n",
    "count = np.zeros([len(columns), len(columns)])\n",
    "\n",
    "# time series는 region 별로 독립적이기 때문에 region_id별로 추출해서 계산해야 한다.\n",
    "# 각 region 별로 correation 값을 계산후 이를 마지막에 평균으로 계산한다. \n",
    "for region_id in df['region_id'].unique():\n",
    "    # region_id (2, 4, 7, 12) 에는 NaN 값이 있기 때문에 해당 데이터들은 제외한다.\n",
    "    if region_id in [2, 4, 7, 12]:\n",
    "        continue\n",
    "\n",
    "    series = df[df['region_id'] == region_id]\n",
    "    series = series[columns]\n",
    "\n",
    "    # correation 값을 계산한다. self-correlation을 계산하면 모든 column 조합의 correlation이 나온다.\n",
    "    corr = np.array(series.corr('pearson'))\n",
    "\n",
    "    # correation 값을 더해준다.\n",
    "    total_corr += corr\n",
    "    count += 1\n",
    "\n",
    "# total_corr를 count로 나눠주면 평균 correlation 값이 나온다.\n",
    "mean_corr = total_corr / count\n",
    "max_index = np.argmax(mean_corr[0][1:])\n",
    "print('Answer:',columns[max_index+1],round(mean_corr[0][max_index+1],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91140, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\giwoong.bae\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\users\\giwoong.bae\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:3515: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._where(-key, value, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 1.661\n"
     ]
    }
   ],
   "source": [
    "# Problem 4\n",
    "\n",
    "# weather 데이터가 local 시간에 맞춰져 있지 않고 표준시이기 때문에 데이터가 shift 되어 있다.\n",
    "# 이에 반해 train 데이터는 local 시간에 맞춰져 있다.\n",
    "# 학습 성능 향상을 위해 weather 데이터를 local 시간에 맞추는 작업을 한다.\n",
    "\n",
    "weather_df = weather_df.reset_index(drop=True)\n",
    "weather_key = ['region_id', 'datetime']\n",
    "temp_series = weather_df[weather_key + ['temperature']].drop_duplicates(subset=weather_key).sort_values(by=weather_key).copy()\n",
    "\n",
    "# 각 region별, 시간별 데이터를 평균낸다.\n",
    "df_temp = temp_series.groupby(['region_id', temp_series.datetime.dt.hour])['temperature'].mean().unstack(level=1)\n",
    "\n",
    "# 기온의 peak가 14시가 되도록 맞춘다. (오후 2시가 가장 덥다는 가정이다.)\n",
    "# peak 값이 14에서 얼마나 떨어져 있는지 offset을 계산한다.\n",
    "region_ids_offsets = pd.Series(df_temp.values.argmax(axis=1) - 14)\n",
    "region_ids_offsets.index.name = 'region_id'\n",
    "\n",
    "# 계산된 값을 weather 데이터의 datetime에서 빼준다. 이때 offset 값을 timedelta hour로 변환해야한다.\n",
    "weather_df_aligned = weather_df.copy()\n",
    "weather_df_aligned['offset'] = weather_df_aligned.region_id.map(region_ids_offsets)\n",
    "weather_df_aligned['datetime'] = (weather_df_aligned.datetime - pd.to_timedelta(weather_df_aligned.offset, unit='H'))\n",
    "\n",
    "train_df_merged_aligned = train_df.merge(weather_df_aligned, on=['region_id', 'datetime'], how='left')                    \n",
    "train_df_merged_aligned = train_df.merge(weather_df_aligned, on=['region_id', 'datetime'], how='left')\n",
    "\n",
    "\n",
    "\n",
    "train_df = train_df_merged_aligned.copy()\n",
    "\n",
    "not_null_list = pd.notnull(train_df['construct_year'])\n",
    "\n",
    "df = train_df.copy()[not_null_list]\n",
    "print(df.shape)\n",
    "\n",
    "# error metric을 Root Mean Squared Logarithmic Error 을 사용한다.\n",
    "df['generate'] = np.log1p(df['generate'])#.astype(np.float32)\n",
    "\n",
    "# datetime을 그대로 사용하지 않고 분리해서 사용한다.\n",
    "# 전체 시간에 대해 regression하는 것보다 월별, 일별, 시간별로 regression 하는게 더 정확하다.\n",
    "df['hour'] = np.uint8(df['datetime'].dt.hour)\n",
    "df['day'] = np.uint8(df['datetime'].dt.day)\n",
    "df['month'] = np.uint8(df['datetime'].dt.month)\n",
    "\n",
    "# datetime 칼럼을 제거한다.\n",
    "for col in df.columns:\n",
    "    if col in ['datetime']:\n",
    "        del df[col]\n",
    "    \n",
    "df_train = df\n",
    "\n",
    "# feature들을 따로 빼준다.\n",
    "# 먼저 예측할 값인 generate를 제외하고 추가로 categorical feature들인 'user_id', 'usage', 'region_id'를 제거한다.\n",
    "# 위에서 offset 칼럼이 생겼기 때문에 이 또한 제거한다. \n",
    "# 최종적으로 사용되는 feature들은 아래와 같다.\n",
    "# ['area', 'construct_year', 'num_floors', 'temperature', 'cloud', 'dew_point', 'rain_hourly', 'air_pressure',\n",
    "# 'wind_direction', 'hour', 'day', 'month']\n",
    "target = 'generate'\n",
    "categorical_features = ['user_id', 'usage', 'region_id']\n",
    "feature_cols = [col for col in df_train.columns if col not in [target, 'offset'] + categorical_features]\n",
    "# print('Used features:', feature_cols)\n",
    "\n",
    "features = df_train[feature_cols]\n",
    "\n",
    "# Nan인 값들은 0으로 채워준다.\n",
    "features[features.isnull()] = 0\n",
    "    \n",
    "label = df_train[target]\n",
    "\n",
    "train_features = features.copy()\n",
    "train_label = label.copy()\n",
    "\n",
    "# Linear Regression\n",
    "clf = LinearRegression(fit_intercept=True, normalize=False)\n",
    "clf.fit(train_features, train_label)\n",
    "\n",
    "# train 데이터 에러값을 계산하고 위해 train 데이터의 feature들을 이용하여 predict 한다.\n",
    "predicted = clf.predict(train_features)\n",
    "expected = label\n",
    "\n",
    "pd.DataFrame.from_dict({\n",
    "    'features': feature_cols,\n",
    "    'std': train_features.std(),\n",
    "    'coef': clf.coef_,\n",
    "    'importance': np.abs(clf.coef_) * train_features.std()\n",
    "})\n",
    "\n",
    "# RMSE 출력\n",
    "print('Answer:',round(np.sqrt(np.mean((predicted - expected) ** 2)),3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 1.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\giwoong.bae\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\users\\giwoong.bae\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:3515: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._where(-key, value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Problem 5\n",
    "\n",
    "train_df = train_df_merged_aligned.copy()\n",
    "\n",
    "df = train_df.copy()[not_null_list]\n",
    "\n",
    "df['generate'] = np.log1p(df['generate'])#.astype(np.float32)\n",
    "\n",
    "df['hour'] = np.uint8(df['datetime'].dt.hour)\n",
    "df['day'] = np.uint8(df['datetime'].dt.day)\n",
    "df['month'] = np.uint8(df['datetime'].dt.month)\n",
    "\n",
    "for col in df.columns:\n",
    "    if col in ['datetime', 'row_id']:\n",
    "        del df[col]\n",
    "    \n",
    "# usage는 categorical feature이기 때문에 regression에 사용할 수 없다.\n",
    "# usage를 feature로 사용하기 위해 usage를 continuous 값으로 변환한다.\n",
    "# 각각의 usage 별로 generate 값의 평균을 내고 해당 평균을 usage 값으로 사용한다.\n",
    "# 이 과정을 log 도메인에서 진행해야 한다. (log1p 함수 이후에 수행해야함)\n",
    "usage_scale = df.groupby('usage', as_index=False)['generate'].mean()\n",
    "usage_scale.columns = ['usage', 'usage_numeric']\n",
    "df = pd.merge(df, usage_scale, how='left', on='usage')\n",
    "\n",
    "# area와 num_floors feature들을 log 도메인으로 변환시켜준다.\n",
    "df['area'] = np.log1p(df['area'])\n",
    "df['num_floors'] = np.log1p(df['num_floors'])\n",
    "    \n",
    "df_train = df\n",
    "\n",
    "target = 'generate'\n",
    "categorical_features = ['user_id', 'usage', 'region_id']\n",
    "feature_cols = [col for col in df_train.columns if col not in [target, 'offset'] + categorical_features]\n",
    "\n",
    "features = df_train[feature_cols]\n",
    "\n",
    "features[features.isnull()] = 0\n",
    "    \n",
    "label = df_train[target]\n",
    "\n",
    "train_features = features.copy()\n",
    "train_label = label.copy()\n",
    "\n",
    "clf = Ridge(alpha=0.001, max_iter=1000, tol=1e-3, random_state=1234)\n",
    "clf.fit(train_features, train_label)\n",
    "\n",
    "predicted = clf.predict(train_features)\n",
    "expected = label\n",
    "\n",
    "pd.DataFrame.from_dict({\n",
    "    'features': feature_cols,\n",
    "    'std': train_features.std(),\n",
    "    'coef': clf.coef_,\n",
    "    'importance': np.abs(clf.coef_) * train_features.std()\n",
    "})\n",
    "\n",
    "print('Answer:',round(np.sqrt(np.mean((predicted - expected) ** 2)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
